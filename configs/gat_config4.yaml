# Memory-efficient GAT Configuration

data:
  root: "./data/processed"
  train_ratio: 0.6
  val_ratio: 0.2
  test_ratio: 0.2
  stratify: true

# Optimized GAT Configuration
model:
  type: "standard"
  hidden_channels: 320  # Increased
  num_layers: 3
  heads: [12, 8, 2]  # More attention heads
  dropout: 0.1  # Reduced since not overfitting
  edge_dim: 1
  residual: true
  layer_norm: true
  feature_augmentation: true  # Turn back on
  pool_type: "mean"
  dtype: "float32"

training:
  epochs: 300  # More epochs
  learning_rate: 0.0005  # Lower starting LR
  weight_decay: 0.0001
  optimizer: "adamw"
  scheduler: "cosine"
  scheduler_warmup: 10  # Add warmup
  gradient_clip: 1.0
  early_stopping_patience: 50  # More patience
  batch_size: 1024
  num_neighbors: [25, 15]  # More neighbors
  accumulation_steps: 2
  mixed_precision: false
  dtype: "float32"

experiment:
  seed: 42
  use_wandb: false
  project_name: "io_performance_gnn"
  run_name: "gat_memory_efficient"

interpretability:
  num_hops: 2
  max_subgraph_size: 500
  n_per_category: 5
  percentiles: [0.25, 0.75]
  top_k_features: 5
  top_k_consensus: 5
  min_consensus_methods: 2
  output_dir: "./analysis/results"
  methods:
    attention: true
    gnn_explainer: true
    gradients: true